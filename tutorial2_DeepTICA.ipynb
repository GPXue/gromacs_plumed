{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DeepTICA: design CVs as slow modes of biased simulations\n",
    "\n",
    "Reference paper: _Bonati, Piccini and Parrinello, [PNAS](https://www.pnas.org/doi/10.1073/pnas.2113533118) (2020)_ [[arXiv]](https://arxiv.org/abs/2107.03943). \n",
    "\n",
    "The aim of this second tutorial is to illustrate how we can design collective variables by analyzing a biased simulation, identifying the modes which hamper its convergence towards the Boltzmann distribution and subsequently biasing them to improve sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collective variables as slow modes**\n",
    "\n",
    "In the framework of the Variational Approach to Conformation Dynamics (VAC), we can relate the eigenfunctions of the transfer operator (which evolves the probability density towards Boltzmann distribution) to the modes that relax more slowly towards the equilibrium. In a rare event scenario, these eigenfunctions are thus connected to the rare transitions between long-lived metastable states. To search for them, we can resort to a variational principle which tells us that these functions are characterized by the highest autocorrelation. Thus, they can be found with a statistical method called Time-lagged independent component analysis (TICA). \n",
    "\n",
    "TICA searches for the linear combinations of input features that are maximally autocorrelated and orthogonal to each other. This amounts to solving the following generalized eigenvalue problem:\n",
    "\n",
    "$$ C(\\tau) w_i = \\lambda_i C(0) w_i$$\n",
    "\n",
    "where $w_i$ are the eigenvectors associated to eigenvalues $\\lambda_i$ and $C_{ij}(\\tau)=<d_i(t)d_j(t+\\tau)>$ is the time-correlation computed at lag-time $\\tau$ between the descriptors.\n",
    "\n",
    "**Deep TICA**\n",
    "\n",
    "As in the previous tutorial, we can use a neural network to learn a feature map of the input into a latent space which allows maximizing the eigenvalues (i.e. the autocorrelation). As can be seen from the figure, the workflow is very similar to the previous DeepLDA method. What changes are the input data (here we need reactive trajectories) and the criterion used to train the NN (to maximize autocorrelation of the output variables rather than to separate the states). Indeed, the loss function here will be to maximize the squared sum of the eigenvalues of the TICA problem, applied to the outputs of the NN.\n",
    "\n",
    "![DeepTICA scheme](images/DeepTICA_scheme.png)\n",
    "\n",
    "**Biased simulations**\n",
    "\n",
    "These methods have been widely exploited to analyze long trajectories of biomolecular systems, made available for example by ad-hoc built computers. However, the main problem in the use of these methods is the need for data on the long-time dynamics of the system, which are rarely available. To overcome this obstacle, we follow a two-step approach. A first exploratory enhanced sampling simulation is performed. This step does not necessarily require the use of a CV-based method and does not even need to be fully converged. It only needs to report on the transition pathways. Then, these enhanced sampling simulations are analyzed and their slow modes are extracted. \n",
    "\n",
    "To apply these methods not only to unbiased simulations but also to enhanced sampling we need to reweight the trajectory to account for the presence of the bias potential. This leads to considering an instantaneous acceleration of the timescales due to the bias potential. Note however that this requires some care from a numerical perspective, as we need to compute the correlation functions of an *exponentially* unevenly spaced time series.\n",
    "\n",
    "An important point to note is the following: even when performing such reweighting, what we will learn are the slow modes of the biased dynamics towards the Boltzmann distribution. These might differ from the one of an unbiased simulation. For instance, if a variable has been accelerated by the addition of bias potential, the signal of such variable in the slow modes might be weaker. For these reasons, we recommend biasing these slow modes in addition to the previous bias potential. This can be also achieved by using a static bias obtained at the end of the first simulation, without the need of optimizing a multi-dimensional bias.\n",
    "\n",
    "**Outline**\n",
    "\n",
    "The three main steps of this method are the following:\n",
    "1. Perform an enhanced sampling calculation using some approximate CVs as well as generalized ensembles simulations (e.g. multicanonical).\n",
    "2. Use this trajectory to train a neural-network to approximate the maximally autocorrelated modes, which correspond to the slowest degrees of freedom.\n",
    "3. Bias these slow degrees of freedom to improve sampling\n",
    "\n",
    "As before, we focus once again on the **alanine dipeptide** molecule. We will start by biasing it using a very bad CV (the $\\psi$ angle), which is often used as a prototypical example of a CV not to be used. Then we will approach it differently, by performing a multithermal simulation in which we want to sample all the conformation relevant to different temperatures, through a bias applied to the potential energy. In both cases we will show how, starting from suboptimal simulations, we can extract the slow modes and consequently accelerate them to greatly improve sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcvs\n",
    "from mlcvs.utils.io import load_dataframe\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def clean(folder='./'):\n",
    "    subprocess.run(\"rm bck.* COLVAR* KERNELS* STATE* DELTAFS alanine*\", cwd=folder, shell=True)\n",
    "    \n",
    "def execute(command, folder):\n",
    "    cmd = subprocess.run(command, cwd=folder, shell=True, capture_output = True, text=True)\n",
    "    if cmd.returncode == 0:\n",
    "        print(f'Completed: {command}')\n",
    "    else:\n",
    "        print(cmd.stderr)\n",
    "\n",
    "GMX_CMD = '. /work/sourceme.sh && gmx_mpi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few plotting functions\n",
    "\n",
    "def plot_ramachandran(x,y,z,scatter=None, ax=None):\n",
    "    # Setup plot\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(5,4.), dpi=100)\n",
    "        ax.set_title(f'Ramachandran plot')\n",
    "\n",
    "    # Plot countour plot\n",
    "    h = ax.hexbin(x,y,C=z,cmap='fessa')\n",
    "    cbar = plt.colorbar(h,ax=ax)\n",
    "    cbar.set_label(f'Deep-TICA')\n",
    "\n",
    "    axs[0].set_xlabel(r'$\\phi$ [rad]')\n",
    "    axs[0].set_ylabel(r'$\\psi$ [rad]')\n",
    "\n",
    "def plot_cv_histogram(s,label=None,ax=None,**kwargs):\n",
    "    # Setup plot\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(5,4.), dpi=100)\n",
    "        ax.set_title('Histogram')\n",
    "\n",
    "    if (type(s)==torch.Tensor):\n",
    "        s = s.squeeze(1).detach().numpy()\n",
    "\n",
    "    # Plot histogram\n",
    "    ax.hist(s,**kwargs)\n",
    "    if label is not None:\n",
    "        ax.set_xlabel(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Deep-TICA CVs from a (very) poor CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial simulation: $\\psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/alanine.png\" width=300 align=\"center\"/>\n",
    "\n",
    "Let's imagine that, by looking at the alanine dipeptide molecule, we guessed the torsion angle $\\psi$ to be associated with a slow mode of the system. We have thus decided to perform a simulation biasing it.\n",
    "\n",
    "Below you can find the input file to perform such a simulation using OPES. However, since we need to simulate it for a long time (several microseconds) to observe some transitions and this requires time, you will find the outputs of such simulation already in the folder.\n",
    "\n",
    "As in the previous tutorial, to stress test the method we will use a sub-optimal set of descriptors, namely the interatomic distances between heavy atoms, and let the neural network do the job to find a non-linear combination of them that is appropriate to describe the slow modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/1_psi/1_opes-psi/'\n",
    "\n",
    "# write plumed input\n",
    "with open(folder+\"plumed.dat\",\"w\") as f:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# Compute torsion angles, as well as energy\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "theta: TORSION ATOMS=6,5,7,9\n",
    "xi: TORSION ATOMS=16,15,17,19\n",
    "ene: ENERGY\n",
    "\n",
    "# Compute descriptors\n",
    "INCLUDE FILE=../../plumed-distances.dat\n",
    "\n",
    "# Define bias\n",
    "opes: OPES_METAD ARG=psi BARRIER=40 SIGMA=0.15 PACE=500 STATE_WFILE=STATE_PSI\n",
    "\n",
    "# Print \n",
    "PRINT FMT=%g STRIDE=2500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the time evolution of $\\psi$ we see that this is indeed a poor choice of the CV. We need to wait for microseconds before observing a handful of transitions. Nevertheless, we want to use this data to extract a better CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/1_psi/1_opes-psi/'\n",
    "colvar = load_dataframe(folder+'COLVAR',stop=400000,stride=5)\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,4),dpi=100)\n",
    "# Time evolution (phi)\n",
    "colvar.plot.scatter('time','psi',s=1,ax=axs[0],cmap='fessa')\n",
    "axs[0].set_title('Time evolution of $\\phi$')\n",
    "# c(t) evution\n",
    "colvar.plot('time','opes.rct',ax=axs[1])\n",
    "axs[1].set_title('Convergence of the bias potential')\n",
    "axs[1].set_ylabel('c(t)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the time evolution of the c(t) quantity in the COLVAR file we see that also in this case the opes bias converges quickly, and this implies that the transitions that we observe are in a quasi-static regime, from which a standard umbrella-like reweighting can be applied. We will discard the first part of the simulation and analyze the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DeepTICA CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "folder = '2_DeepTICA/1_psi/1_opes-psi/'\n",
    "colvar = load_dataframe(folder+'COLVAR',start=10000,stop=40000,stride=1) # we discard the first 500 ns\n",
    "\n",
    "# Select descriptors\n",
    "X = colvar.filter(regex='d_').values\n",
    "t = colvar['time'].values\n",
    "n_input = X.shape[1]\n",
    "\n",
    "print(X.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute weights for time rescaling**\n",
    "\n",
    "In order to compute the autocorrelation functions from a biased simulation we first need to  rescale the time to account for the effect of the bias potential. Since we are in a quasi-static regime, we can simply use the following umbrella-sampling-like formula for the weights:\n",
    "\n",
    "$$ w = e^{\\beta\\ V} $$\n",
    "\n",
    "From the weights we  then compute the instantaneus acceleration at step $k$ as:\n",
    "\n",
    "$$ dt'_k = w_k\\ dt $$\n",
    "\n",
    "and hence the cumulative rescaled time as:\n",
    "\n",
    "$$ t'_k = \\sum_{i=0} ^k dt'_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 300.        #reweighting temperature\n",
    "\n",
    "kb=0.008314\n",
    "kbt=kb*temp\n",
    "\n",
    "# compute logweights\n",
    "bias = colvar.filter(regex='.bias').values.sum(axis=1) # Load all *.bias columns and sum them\n",
    "logweight = bias/kbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dataset of time-lagged configurations**\n",
    "\n",
    "In order to train the Deep-TICA CVs we will need to compute the time-lagged covariance matrices in the rescaled time $t'$. The standard way is to look for configurations which are distant a lag-time $\\tau$ in the time series. However, in the rescaled time the time-series is _exponentially_ unevenly spaced. Hence, a naive search will lead to severe numerical issue. To address this, we resort to the algorithm described in (Yang and Parrinello, _Journal of chemical theory and computation_ (2018)). \n",
    "\n",
    "To generate the the training and validation set, we use the function `create_time_lagged_dataset` which searches for the pairs of configurations and the corresponding weight. This is divided in training and validation data, and fed to a `FastTensorDataloader` which allows for an efficient training.\n",
    "\n",
    "A note on the choice of the lag-time: if we were to apply this method to an unbiased simulation, the meaning of $\\tau$ would be to look at correlations on timescales greater than it. Hovewer, when rescaling the time to account for the bias potential we lose this perspective. So in order to choose it we can train a model with different lag-times and choose look for the following two requirements:\n",
    "- all the eigenvalues you want to optimize are not decayed to zero (avoid values too large)\n",
    "- the eigenvalues are not degenerate (avoid too small values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.data import create_time_lagged_dataset, FastTensorDataLoader\n",
    "from torch.utils.data import Subset,random_split\n",
    "\n",
    "lag_time = 10\n",
    "\n",
    "# create dataset\n",
    "dataset = create_time_lagged_dataset(X,t=t,\n",
    "                                       lag_time=lag_time,\n",
    "                                       logweights=logweight)\n",
    "\n",
    "n_train  = int( 0.8 * len(dataset) )\n",
    "n_valid  = len(dataset) - n_train\n",
    "\n",
    "# split train - valid \n",
    "train_data = Subset(dataset, np.arange(n_train))\n",
    "valid_data = Subset(dataset, np.arange(n_train,n_train+n_valid))\n",
    "\n",
    "# create dataloaders \n",
    "train_loader = FastTensorDataLoader(train_data)\n",
    "valid_loader = FastTensorDataLoader(valid_data)\n",
    "\n",
    "print('Time-lagged pairs:\\t',len(dataset))\n",
    "print('Training data:\\t\\t',len(train_data))\n",
    "print('Validation data:\\t',len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the DeepTICA CVs**\n",
    "\n",
    " Several parameters are similar to the the ones used in the training of DeepLDA, with the exception of `loss_type` and `n_eig`. They are relative at the objective function that is being maximized. \n",
    "\n",
    "Here `loss_type` specifies the object that is maximized in the loss, and it can be:\n",
    "- \"sum2\": sum of square of the eigenvalues\n",
    "- \"sum\": sum of the eigenvalues\n",
    "- \"single\": only a single eigenvalue (which eigenvalue is specified by `n_eig`)\n",
    "\n",
    "In the following we train a model to reconstruct the slowest DeepTICA 1 CV (using `loss_type=sum2` and `n_eig=1`).\n",
    "\n",
    "---\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :- | :- | :- |\n",
    "| **Neural network** |\n",
    "| nodes | list | NN architecture (last value equal to the number of hidden layers which are input of TICA)  |\n",
    "| activ_type | string | Activation function (relu,tanh,elu,linear) |\n",
    "| **loss_type** | string | Loss function operating on the TICA eigenvalues (sum2,sum,single)|\n",
    "| **n_eig** | int | Number of eigenvalues to optimize (or if loss_type=single which one to select) |\n",
    "| **Optimization** |\n",
    "| lrate | float | Learning rate |\n",
    "| num_epochs | int | Number of epochs |\n",
    "| **Early Stopping** |\n",
    "| earlystop | bool | Whether to use early stopping based on validation loss |\n",
    "| es_patience | int | Number of epochs before stopping |\n",
    "| es_consecutive | bool | Whether es_patience should count consecutive (True) or cumulative patience |\n",
    "| **Log** |\n",
    "| log_every | int | How often print the train/valid loss during training |\n",
    "\n",
    "See class documentation for further details about parameters and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.tica import DeepTICA_CV\n",
    "\n",
    "#------------- PARAMETERS -------------\n",
    "nodes             = [n_input,30,30,2]\n",
    "activ_type        = 'tanh'\n",
    "loss_type         = 'sum2'\n",
    "n_eig             = 1\n",
    "\n",
    "lrate             = 1e-3\n",
    "\n",
    "num_epochs        = 1000\n",
    "earlystop         = True\n",
    "es_patience       = 10\n",
    "\n",
    "log_every         = 10\n",
    "#--------------------------------------\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MODEL\n",
    "model = DeepTICA_CV(nodes)\n",
    "model.to(device)\n",
    "\n",
    "# OPTIMIZER\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "model.set_optimizer(opt)\n",
    "model.set_earlystopping(patience=es_patience,min_delta=0.001, log=False)\n",
    "\n",
    "# TRAIN\n",
    "model.fit(train_loader,valid_loader,\n",
    "            standardize_inputs=True,\n",
    "            standardize_outputs=True,\n",
    "            loss_type=loss_type,\n",
    "            n_eig=n_eig,\n",
    "            nepochs=num_epochs,\n",
    "            info=False, log_every=log_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the learning curve and also see the evolution of the eigenvalues along the training (interesting especially if there is more than one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(12,5),dpi=100)\n",
    "\n",
    "loss_train = [x.cpu() for x in model.loss_train]\n",
    "loss_valid = [x.cpu() for x in model.loss_valid]\n",
    "\n",
    "# Loss function\n",
    "ax = axs[0]\n",
    "ax.plot(loss_train,'-',label='Train')\n",
    "ax.plot(loss_valid,'--',label='Valid')\n",
    "ax.set_ylabel('Loss Function')\n",
    "\n",
    "# Eigenvalues vs epoch\n",
    "ax = axs[1]\n",
    "with torch.no_grad():\n",
    "    evals_train = np.asarray(torch.cat(model.evals_train).cpu())\n",
    "for i in range(n_eig):\n",
    "    ax.plot(evals_train[:,i],label='Eig. '+str(i+1))\n",
    "ax.set_ylabel('Eigenvalues')\n",
    "\n",
    "# Common setup\n",
    "for ax in axs:\n",
    "    if model.earlystopping_.early_stop:\n",
    "        ax.axvline(model.earlystopping_.best_epoch,ls='dotted',color='grey',alpha=0.5,label='Early Stopping')\n",
    "        ax.set_xlabel('#Epochs')\n",
    "        ax.legend(ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we understand what is the physical meaning of the different CVs? One possibility is to check how the DeepTICA CVs changes in a physical space, e.g. the ramachandran plot ($\\phi,\\psi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbin plot in physical space    \n",
    "fig,axs = plt.subplots(1,n_eig,figsize=(6*n_eig,5),dpi=100)\n",
    "\n",
    "if n_eig ==1:\n",
    "    axs = [axs]\n",
    "    \n",
    "x = colvar['phi'].values\n",
    "y = colvar['psi'].values\n",
    "\n",
    "# compute cvs\n",
    "with torch.no_grad():\n",
    "    s = model(torch.Tensor(X)).numpy()\n",
    "    \n",
    "for i,ax in enumerate(axs):\n",
    "    pp = ax.hexbin(x,y,C=s[:,i],gridsize=150,cmap='fessa')\n",
    "    cbar = plt.colorbar(pp,ax=ax)\n",
    "    ax.set_title('Deep-TICA '+str(i+1))\n",
    "    ax.set_xlabel(r'$\\phi$ [rad]')\n",
    "    ax.set_ylabel(r'$\\psi$ [rad]')\n",
    "    cbar.set_label('Deep-TICA '+str(i+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** : train multiple CVs\n",
    "\n",
    "Repeat the training by changing `loss_type` and/or `n_eig` to optimize also the second eigenvalue. \n",
    "\n",
    "- Are the results related to the first DeepTICA 1 CV robust? And what about the second one? Is the value of the lag-time used suitable to recover also the second DeepTICA 2 CV?\n",
    "\n",
    "- How can we understand if the two CVs are related to slow or fast modes? Try to plot also time evolution of the two DeepTICA CVs along the initial trajectory.\n",
    "\n",
    "- Based on the free energy profile of alanine dipeptide, what would you expect to be the second slowest mode? Do you have any explanation why we are able to detect it in this simulation?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.tica import DeepTICA_CV\n",
    "\n",
    "#------------- PARAMETERS -------------\n",
    "nodes             = [n_input,30,30,2]\n",
    "activ_type        = 'tanh'\n",
    "loss_type         = _____FILL_____\n",
    "n_eig             = _____FILL_____\n",
    "\n",
    "lrate             = 1e-3\n",
    "l2_reg            = 0.\n",
    "\n",
    "num_epochs        = 1000\n",
    "earlystop         = True\n",
    "es_patience       = 10\n",
    "\n",
    "log_every         = 10\n",
    "#--------------------------------------\n",
    "\n",
    "# MODEL\n",
    "model = DeepTICA_CV(nodes)\n",
    "model.to(device)\n",
    "\n",
    "# OPTIMIZER\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
    "model.set_optimizer(opt)\n",
    "model.set_earlystopping(patience=es_patience,min_delta=0.001, log=False)\n",
    "\n",
    "# TRAIN\n",
    "model.fit(train_loader,valid_loader,\n",
    "            standardize_inputs=True,\n",
    "            standardize_outputs=True,\n",
    "            loss_type=loss_type,\n",
    "            n_eig=n_eig,\n",
    "            nepochs=num_epochs,\n",
    "            info=False, log_every=log_every)\n",
    "\n",
    "# Hexbin plot in physical space    \n",
    "fig,axs = plt.subplots(1,n_eig,figsize=(6*n_eig,5),dpi=100)\n",
    "\n",
    "if n_eig ==1:\n",
    "    axs = [axs]\n",
    "    \n",
    "x = colvar['phi'].values\n",
    "y = colvar['psi'].values\n",
    "\n",
    "# compute cvs\n",
    "with torch.no_grad():\n",
    "    s = model(torch.Tensor(X)).numpy()\n",
    "    \n",
    "for i,ax in enumerate(axs):\n",
    "    pp = ax.hexbin(x,y,C=s[:,i],gridsize=150,cmap='fessa')\n",
    "    cbar = plt.colorbar(pp,ax=ax)\n",
    "    ax.set_title('Deep-TICA '+str(i+1))\n",
    "    ax.set_xlabel(r'$\\phi$ [rad]')\n",
    "    ax.set_ylabel(r'$\\psi$ [rad]')\n",
    "    cbar.set_label('Deep-TICA '+str(i+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values of the DeepTICA CVs along the initial trajectory\n",
    "with torch.no_grad():\n",
    "    input = torch.Tensor(_____FILL____)\n",
    "    s = model(input).cpu().numpy()\n",
    "\n",
    "fig,axs = plt.subplots(1,n_eig,figsize=(6*n_eig,4),dpi=100)\n",
    "\n",
    "if n_eig == 1 :\n",
    "    axs = [axs]\n",
    "\n",
    "for i in range(n_eig):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(f'DeepTICA {i+1}')\n",
    "    ax.plot(colvar['time'],s[:,i])\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'DeepTICA {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back and train a model to approximate only the slowest DeepTICA 1 CV, which we will will use in the next step to enhance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bias Deep-TICA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have analyzed the initial simulation and extracted the slow modes into the DeepTICA CVs, we can perform a new OPES simulation biasing the leading DeepTICA 1 CV, in addition to the bias from the previous simulation (static).     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/1_psi/2_deeptica1/'\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "execute(f\"cp ../../md_inputs/input* .\", folder=folder)\n",
    "\n",
    "# export model\n",
    "model.export(folder)\n",
    "\n",
    "# write plumed input\n",
    "with open(folder+\"plumed.dat\",\"w\") as f:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# Compute torsion angles, as well as energy\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "theta: TORSION ATOMS=6,5,7,9\n",
    "xi: TORSION ATOMS=16,15,17,19\n",
    "ene: ENERGY\n",
    "\n",
    "# Compute descriptors\n",
    "INCLUDE FILE=../../plumed-distances.dat\n",
    "\n",
    "# Compute DeepTICA CVs\n",
    "deep: PYTORCH_MODEL FILE=model.ptc ARG=d_2_5,d_2_6,d_2_7,d_2_9,d_2_11,d_2_15,d_2_16,d_2_17,d_2_19,d_5_6,d_5_7,d_5_9,d_5_11,d_5_15,d_5_16,d_5_17,d_5_19,d_6_7,d_6_9,d_6_11,d_6_15,d_6_16,d_6_17,d_6_19,d_7_9,d_7_11,d_7_15,d_7_16,d_7_17,d_7_19,d_9_11,d_9_15,d_9_16,d_9_17,d_9_19,d_11_15,d_11_16,d_11_17,d_11_19,d_15_16,d_15_17,d_15_19,d_16_17,d_16_19,d_17_19\n",
    "\n",
    "# Apply OPES bias (combine with the static bias equal to the one obtained at the end of the first simulation)\n",
    "static: OPES_METAD RESTART=YES FILE=KERNELS_PSI ARG=psi BARRIER=40 SIGMA=0.15 PACE=50000000 STATE_RFILE=../1_opes-psi/STATE_PSI\n",
    "opes: OPES_METAD ARG=deep.node-0 BARRIER=30 PACE=500 STATE_WFILE=STATE\n",
    "\n",
    "# Print \n",
    "PRINT FMT=%g STRIDE=500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=f)\n",
    "\n",
    "## RUN GROMACS\n",
    "num_steps=2500000\n",
    "\n",
    "clean(folder)\n",
    "execute(f\"{GMX_CMD} mdrun -s input.tpr -deffnm alanine -plumed plumed.dat -ntomp 1 -nsteps {num_steps} > alanine.out\", folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the calculation is over we can analyze the trajectory as in the previous tutorial. First we look at the time evolution of the DeepTICA CV along the trajectory, as well as plotting the Ramachandran plot to see the explored space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/1_psi/2_deeptica1/'\n",
    "colvar = load_dataframe(folder+'COLVAR')\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,4),dpi=100)\n",
    "# Time evolution (DeepTICA)\n",
    "colvar.plot.scatter('time','deep.node-0',s=1,ax=axs[0])\n",
    "axs[1].set_xlabel('Time [ps]')\n",
    "axs[1].set_ylabel('DeepTICA 1')\n",
    "# 2D scatter plot colored with DeepTICA\n",
    "colvar.plot.scatter('phi','psi',c='deep.node-0',s=1,cmap='fessa',ax=axs[1])\n",
    "axs[1].set_xlabel(r'$\\phi$ [rad]')\n",
    "axs[1].set_ylabel(r'$\\psi$ [rad]')\n",
    "axs[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.fes import compute_fes\n",
    "\n",
    "s = colvar['deep.node-0'].values\n",
    "\n",
    "# compute weights\n",
    "kbT = 2.5\n",
    "w = np.exp(colvar.filter(regex='.bias').values.sum(axis=1)/kbT)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,4),dpi=100)\n",
    "fes,grid,bounds,error = compute_fes(s, weights=w, kbt=kbT, \n",
    "                                    blocks=4, bandwidth=0.02, \n",
    "                                    plot=True, ax = ax)\n",
    "ax.set_xlabel('DeepTICA 1')\n",
    "ax.set_ylabel('FES [kJ/mol]')\n",
    "ax.set_ylim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this tutorial we can compute the correlation with a set of physical descriptors, to obtain a physical insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select deeptica and input distances, as well as dihedral angles\n",
    "cols = ['deep.node-0', 'phi', 'psi', 'theta', 'xi' ]\n",
    "\n",
    "# compute correlation\n",
    "corr = colvar[cols].corr(method='pearson')\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(figsize=(4,4),dpi=100)\n",
    "corr['deep.node-0'].drop('deep.node-0').plot(kind='bar', ax=ax, rot=35)\n",
    "ax.set_ylabel('Correlation with DeepTICA 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bonus: bias Deep-TICA 1 alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the introduction, the DeepTICA CVs that we find are connected to the slow modes of the initial biased simulation. So in general it is appropriate to accelerate them together with the bias used in the first run. You can try to see what happens if you bias only the DeepTICA 1 CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deep-TICA CVs from a multithermal simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second exercise we want to learn the CVs from a different kind of simulation. Instead of performing the initial simulation biasing another CV, we resort to a multithermal simulation.\n",
    "This amount to sampling the configurations relevant to a set of temperatures, and it is the same ensemble simulated with parallel tempering/replica exchange methods. However, with OPES we can perform such simulation with a single replica by biasing the potential energy. See also the Masterclass 22.03 to learn more.\n",
    "\n",
    "This approach has the advantage that we do not have to start with a guess of a CV in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial simulation: multithermal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case you can find already a multicanonical simulation in the folder, performed with the following input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/2_multithermal/1_multi/'\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "execute(f\"cp ../md_inputs/input* .\", folder=folder)\n",
    "\n",
    "# write plumed input\n",
    "with open(folder+\"plumed.dat\",\"w\") as f:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# Compute torsion angles, as well as energy\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "theta: TORSION ATOMS=6,5,7,9\n",
    "xi: TORSION ATOMS=16,15,17,19\n",
    "ene: ENERGY\n",
    "\n",
    "# Compute descriptors\n",
    "INCLUDE FILE=../../plumed-distances.dat\n",
    "\n",
    "# Define multithermal bias\n",
    "ecv: ECV_MULTITHERMAL ARG=ene TEMP_MAX=600\n",
    "opes: OPES_EXPANDED ARG=ecv.ene PACE=500\n",
    "\n",
    "# Print \n",
    "PRINT FMT=%g STRIDE=500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=f)\n",
    "\n",
    "## RUN GROMACS\n",
    "num_steps=25000000 #50ns\n",
    "\n",
    "#clean(folder)\n",
    "#execute(f\"{GMX_CMD} mdrun -s input.tpr -deffnm alanine -plumed plumed.dat -ntomp 1 -nsteps {num_steps}\", folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multithermal simulation is much more efficient than the previous $\\psi$-based simulation. Still, the sampling is far from optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/2_multithermal/1_multi/'\n",
    "colvar = load_dataframe(folder+'COLVAR')\n",
    "deltafs = load_dataframe(folder+'DELTAFS')\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,4),dpi=100)\n",
    "# Time evolution (phi)\n",
    "colvar.plot.scatter('time','phi',c='ene',s=1,ax=axs[0],cmap='fessa')\n",
    "axs[0].set_title('Time evolution of $\\phi$')\n",
    "# c(t) evolution\n",
    "deltafs.plot('time','rct',ax=axs[1])\n",
    "axs[1].set_title('Convergence of the bias potential')\n",
    "axs[1].set_ylabel('c(t)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DeepTICA CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "folder = '2_DeepTICA/2_multithermal/1_multi/'\n",
    "colvar = load_dataframe(folder+'COLVAR',start=15000) #discard first 15 ns\n",
    "\n",
    "# Select descriptors\n",
    "X = colvar.filter(regex='d_').values\n",
    "n_input = X.shape[1]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute weights for time rescaling**\n",
    "\n",
    "Since the initial simulation is performed in the multicanonical ensemble, we need to reweight the points in a different way.\n",
    "\n",
    "We first extract the time $t$, the energy $E$ (needed for the multicanonical reweight [1]) and the bias $V$ from the COLVAR file. We then calculate the weights as:\n",
    "\n",
    "$$ w = e^{\\beta\\ V + (\\beta_0-\\beta)\\ E} $$\n",
    "\n",
    "NB: if simulation temperature $\\beta_0$ is equal to the reweighting one $\\beta$ the multicanonical reweight coincides with the standard umbrella sampling-like case.\n",
    "\n",
    "Once we have computed the weights, we rescale the time as before by usingas before using: $ dt'_k = w_k\\ dt $.\n",
    "\n",
    "\n",
    "[1] Invernizzi, Piaggi, and Parrinello. \"Unified approach to enhanced sampling.\" _Physical Review X_ 10.4 (2020): 041034.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- PARAMETERS -------------\n",
    "multicanonical    = True        #using a standard reweight (false) or a multicanonical one (true)\n",
    "temp              = 300.        #reweighting temperature\n",
    "temp0             = 300.        #simulation temperature (only needed if multicanonical == True)\n",
    "#--------------------------------------\n",
    "\n",
    "# Calculate inverse temperature\n",
    "kb=0.008314\n",
    "beta=1./(kb*temp)\n",
    "beta0=1./(kb*temp0)\n",
    "\n",
    "# Extract cvs from df\n",
    "\n",
    "t = colvar['time'].values # save time\n",
    "ene = colvar['ene'].values.astype(np.float64) # store energy as long double\n",
    "bias = colvar.filter(regex='.bias').values.sum(axis=1) # Load *.bias columns and sum them\n",
    "\n",
    "# Compute log-weights for time reweighting\n",
    "logweight = beta*bias\n",
    "\n",
    "if multicanonical:\n",
    "    ene -= np.mean(ene) #first shift energy by its mean value\n",
    "    logweight += (beta0-beta)*ene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset of time-lagged configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.data import create_time_lagged_dataset, FastTensorDataLoader\n",
    "from torch.utils.data import Subset,random_split\n",
    "\n",
    "lag_time = 1.0\n",
    "\n",
    "# create dataset\n",
    "dataset = create_time_lagged_dataset(X,t=t,\n",
    "                                       lag_time=lag_time,\n",
    "                                       logweights=logweight)\n",
    "\n",
    "n_train  = int( 0.8 * len(dataset) )\n",
    "n_valid  = len(dataset) - n_train\n",
    "train_data, valid_data = random_split(dataset,[n_train,n_valid]) \n",
    "\n",
    "# create dataloaders \n",
    "train_loader = FastTensorDataLoader(train_data)\n",
    "valid_loader = FastTensorDataLoader(valid_data)\n",
    "\n",
    "print('Time-lagged pairs:\\t',len(dataset))\n",
    "print('Training data:\\t\\t',len(train_data))\n",
    "print('Validation data:\\t',len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will optimize directly two DeepTICA CVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.tica import DeepTICA_CV\n",
    "\n",
    "#------------- PARAMETERS -------------\n",
    "nodes             = [n_input,30,30,2]\n",
    "activ_type        = 'tanh'\n",
    "loss_type         = 'sum2'\n",
    "n_eig             = 2\n",
    "\n",
    "lrate             = 1e-3\n",
    "l2_reg            = 0.\n",
    "\n",
    "num_epochs        = 1000\n",
    "earlystop         = True\n",
    "es_patience       = 10\n",
    "es_consecutive    = False\n",
    "\n",
    "log_every         = 10\n",
    "#--------------------------------------\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MODEL\n",
    "model = DeepTICA_CV(nodes)\n",
    "model.to(device)\n",
    "\n",
    "# OPTIMIZER\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
    "\n",
    "# REGULARIZATION\n",
    "model.set_optimizer(opt)\n",
    "model.set_earlystopping(patience=es_patience,min_delta=0.,consecutive=es_consecutive, save_best_model=True, log=False)\n",
    "\n",
    "# TRAIN\n",
    "model.fit(train_loader,valid_loader,\n",
    "            standardize_inputs=True,\n",
    "            standardize_outputs=True,\n",
    "            loss_type=loss_type,\n",
    "            n_eig=n_eig,\n",
    "            nepochs=num_epochs,\n",
    "            info=False, log_every=log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(12,5),dpi=100)\n",
    "\n",
    "loss_train = [x.cpu() for x in model.loss_train]\n",
    "loss_valid = [x.cpu() for x in model.loss_valid]\n",
    "\n",
    "# Loss function\n",
    "ax = axs[0]\n",
    "ax.plot(loss_train,'-',label='Train')\n",
    "ax.plot(loss_valid,'--',label='Valid')\n",
    "ax.set_ylabel('Loss Function')\n",
    "\n",
    "# Eigenvalues vs epoch\n",
    "ax = axs[1]\n",
    "with torch.no_grad():\n",
    "    evals_train = np.asarray(torch.cat(model.evals_train).cpu())\n",
    "for i in range(n_eig):\n",
    "    ax.plot(evals_train[:,i],label='Eig. '+str(i+1))\n",
    "ax.set_ylabel('Eigenvalues')\n",
    "\n",
    "# Common setup\n",
    "for ax in axs:\n",
    "    if model.earlystopping_.early_stop:\n",
    "        ax.axvline(model.earlystopping_.best_epoch,ls='dotted',color='grey',alpha=0.5,label='Early Stopping')\n",
    "        ax.set_xlabel('#Epochs')\n",
    "        ax.legend(ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Is the choice of lag-time appropriate to recover not only the slowest CV but also the second one? How are the results changing if you change the lag-time?\n",
    "- Can we understand something more about the second DeepTICA 2 CV in this case? Do you have any idea why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbin plot in physical space    \n",
    "fig,axs = plt.subplots(1,n_eig,figsize=(6*n_eig,5),dpi=100)\n",
    "\n",
    "x = colvar['phi'].values\n",
    "y = colvar['psi'].values\n",
    "\n",
    "# compute cvs\n",
    "with torch.no_grad():\n",
    "    s = model(torch.Tensor(X)).numpy()\n",
    "    \n",
    "for i,ax in enumerate(axs):\n",
    "    pp = ax.hexbin(x,y,C=s[:,i],gridsize=150,cmap='fessa')\n",
    "    cbar = plt.colorbar(pp,ax=ax)\n",
    "    ax.set_title('Deep-TICA '+str(i+1))\n",
    "    ax.set_xlabel(r'$\\phi$ [rad]')\n",
    "    ax.set_ylabel(r'$\\psi$ [rad]')\n",
    "    cbar.set_label('Deep-TICA '+str(i+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we learn if we compute the FES along the extracted DeepTICA CVs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.fes import compute_fes\n",
    "\n",
    "fig,axs = plt.subplots(1,n_eig,figsize=(6*n_eig,5),dpi=100)\n",
    "\n",
    "for i in range(n_eig):\n",
    "    fes,grid,bounds,error = compute_fes(s[:,i], weights=np.exp(logweight),\n",
    "                                        blocks=2,\n",
    "                                        bandwidth=0.02,scale_by='range',\n",
    "                                        plot=True, plot_max_fes=100, ax = axs[i])\n",
    "    axs[i].set_xlabel('Deep-TICA '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bias DeepTICA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bias the slowest DeepTICA CV in the multithermal ensemble. We could do it as before by appling the multithermal bias statically. Instead, we use a multiumbrella ensemble simulation and optimized it together with the multithermal expanded CVs. In this way we show how we can optimize the two together if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/2_multithermal/2_deeptica1/'\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "execute(f\"cp ../../md_inputs/input* .\", folder=folder)\n",
    "\n",
    "model.export(folder)\n",
    "\n",
    "# write plumed input\n",
    "with open(folder+\"plumed.dat\",\"w\") as f:\n",
    "    print(\"\"\"\n",
    "# vim:ft=plumed\n",
    "\n",
    "# Compute torsion angles, as well as energy\n",
    "MOLINFO STRUCTURE=input.ala2.pdb\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "theta: TORSION ATOMS=6,5,7,9\n",
    "xi: TORSION ATOMS=16,15,17,19\n",
    "ene: ENERGY\n",
    "\n",
    "# Compute descriptors\n",
    "INCLUDE FILE=../../plumed-distances.dat\n",
    "\n",
    "# Compute DeepTICA CVS\n",
    "deep: PYTORCH_MODEL FILE=model.ptc ARG=d_2_5,d_2_6,d_2_7,d_2_9,d_2_11,d_2_15,d_2_16,d_2_17,d_2_19,d_5_6,d_5_7,d_5_9,d_5_11,d_5_15,d_5_16,d_5_17,d_5_19,d_6_7,d_6_9,d_6_11,d_6_15,d_6_16,d_6_17,d_6_19,d_7_9,d_7_11,d_7_15,d_7_16,d_7_17,d_7_19,d_9_11,d_9_15,d_9_16,d_9_17,d_9_19,d_11_15,d_11_16,d_11_17,d_11_19,d_15_16,d_15_17,d_15_19,d_16_17,d_16_19,d_17_19\n",
    "\n",
    "# Define multithermal bias and multiumbrellas on deeptica 1\n",
    "ecv: ECV_MULTITHERMAL ARG=ene TEMP_MAX=600\n",
    "umb: ECV_UMBRELLAS_LINE ARG=deep.node-0 SIGMA=0.1 CV_MIN=-1.0 CV_MAX=1.0 BARRIER=35\n",
    "opes: OPES_EXPANDED ARG=ecv.*,umb.* PACE=500\n",
    "\n",
    "# Print \n",
    "PRINT FMT=%g STRIDE=500 FILE=COLVAR ARG=*\n",
    "\n",
    "ENDPLUMED\n",
    "\"\"\",file=f)\n",
    "\n",
    "## RUN GROMACS\n",
    "num_steps=2500000 #5ns\n",
    "\n",
    "clean(folder)\n",
    "execute(f\"{GMX_CMD} mdrun -s input.tpr -deffnm alanine -plumed plumed.dat -ntomp 1 -nsteps {num_steps}\", folder=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2_DeepTICA/2_multithermal/2_deeptica1/'\n",
    "colvar = load_dataframe(folder+'COLVAR')\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,4),dpi=100)\n",
    "# Time evolution (DeepTICA)\n",
    "colvar.plot.scatter('time','deep.node-0',s=1,ax=axs[0])\n",
    "axs[1].set_xlabel('Time [ps]')\n",
    "axs[1].set_ylabel('DeepTICA 1')\n",
    "# 2D scatter plot colored with DeepTICA\n",
    "colvar.plot.scatter('phi','psi',c='deep.node-0',s=1,cmap='fessa',ax=axs[1])\n",
    "axs[1].set_xlabel(r'$\\phi$ [rad]')\n",
    "axs[1].set_ylabel(r'$\\psi$ [rad]')\n",
    "axs[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the FES we need to reweight the simulation. Similarly to what we have done for computing the time-correlation functions, since there is also a multithermal bias we need to reweight accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- PARAMETERS -------------\n",
    "multicanonical    = True        #using a standard reweight (false) or a multicanonical one (true)\n",
    "temp              = 300.        #reweighting temperature\n",
    "temp0             = 300.        #simulation temperature (only needed if multicanonical == True)\n",
    "#--------------------------------------\n",
    "\n",
    "# Calculate inverse temperature\n",
    "kb=0.008314\n",
    "beta=1./(kb*temp)\n",
    "beta0=1./(kb*temp0)\n",
    "\n",
    "# Extract cvs from df\n",
    "\n",
    "t = colvar['time'].values # save time\n",
    "ene = colvar['ene'].values.astype(np.float64) # store energy as long double\n",
    "bias = colvar.filter(regex='.bias').values.sum(axis=1) # Load *.bias columns and sum them\n",
    "\n",
    "# Compute log-weights for time reweighting\n",
    "logweight = beta*bias\n",
    "\n",
    "if multicanonical:\n",
    "    ene -= np.mean(ene) #first shift energy by its mean value\n",
    "    logweight += (beta0-beta)*ene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvs.utils.fes import compute_fes\n",
    "\n",
    "s = colvar['deep.node-0'].values\n",
    "\n",
    "# compute weights\n",
    "kbT = 2.5\n",
    "w = np.exp(logweight)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,4),dpi=100)\n",
    "fes,grid,bounds,error = compute_fes(s, weights=w, kbt=kbT, \n",
    "                                    blocks=5, bandwidth=0.04, \n",
    "                                    plot=True, ax = ax)\n",
    "ax.set_xlabel('DeepTICA 1')\n",
    "ax.set_ylabel('FES [kJ/mol]')\n",
    "ax.set_ylim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select deeptica as well as dihedral angles\n",
    "cols = ['deep.node-0', 'phi', 'psi', 'theta', 'xi' ]\n",
    "\n",
    "# compute correlation\n",
    "corr = colvar[cols].corr(method='pearson')\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(figsize=(4,4),dpi=100)\n",
    "corr['deep.node-0'].drop('deep.node-0').plot(kind='bar', ax=ax, rot=35)\n",
    "ax.set_ylabel('Correlation with DeepTICA 1')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cbeac1d7079eaeba64f3210ccac5ee24400128e300a45ae35eee837885b08b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
